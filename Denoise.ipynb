{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Denoise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d_vyMQF2XbTd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from os.path import join\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Noise(Dataset):\n",
        "    def __init__(self, root, gt_root, transform=None):\n",
        "        \"\"\" Intialize the dataset \"\"\"\n",
        "        self.filenames = []\n",
        "        self.root = root\n",
        "        self.gt_root = gt_root\n",
        "        self.transform = transform\n",
        "\n",
        "        # read filenames\n",
        "        for i in range(1,1152):\n",
        "            blur, gau, sp, poi = join(gt_root,str(i)+'.png'), join(root,str(i)+'_gaussian.png'), join(root,str(i)+'_sp.png'), join(root,str(i)+'_poisson.png')\n",
        "            blur, gau, sp, poi = Image.open(blur), Image.open(gau), Image.open(sp), Image.open(poi)\n",
        "            if self.transform is not None:\n",
        "                blur = self.transform(blur)\n",
        "                gau = self.transform(gau)\n",
        "                sp = self.transform(sp)\n",
        "                poi = self.transform(poi)\n",
        "            self.filenames.append([blur,gau,sp,poi])\n",
        "\n",
        "        self.len = len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Get a sample from the dataset \"\"\"\n",
        "        blur, gau, sp, poi = self.filenames[index]\n",
        "        \n",
        "        return blur, gau, sp, poi\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Total number of samples in the dataset \"\"\"\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "hkYjuREWXokJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = {'state_dict': model.state_dict(),\n",
        "             'optimizer' : optimizer.state_dict()}\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print('model saved to %s' % checkpoint_path)\n",
        "    \n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "    optimizer.load_state_dict(state['optimizer'])\n",
        "    print('model loaded from %s' % checkpoint_path)"
      ],
      "metadata": {
        "id": "v4p52OmBfzzP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import RandomCrop\n",
        "# Create the dataset. \n",
        "# transforms.ToTensor() automatically converts PIL images to\n",
        "trans = transforms.Compose([\n",
        "    #transforms.Resize(128),\n",
        "    transforms.CenterCrop(256),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "NoiseSet = Noise(root='/content/blurred_noise',gt_root='/content/blurred',transform=trans)\n",
        "train_set, val_set = torch.utils.data.random_split(NoiseSet, [900, 251])\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_set, batch_size=8, num_workers=0)"
      ],
      "metadata": {
        "id": "HSIS-qKYY04c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet"
      ],
      "metadata": {
        "id": "NoOxwOcPz7j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The UNet model credit to jakeoung\n",
        "https://github.com/jakeoung/Unet_pytorch/blob/master/model.py\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def add_conv_stage(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True, useBN=False):\n",
        "  if useBN:\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.BatchNorm2d(dim_out),\n",
        "      nn.LeakyReLU(0.1),\n",
        "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.BatchNorm2d(dim_out),\n",
        "      nn.LeakyReLU(0.1)\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "      nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "def add_merge_stage(ch_coarse, ch_fine, in_coarse, in_fine, upsample):\n",
        "  conv = nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
        "  torch.cat(conv, in_fine)\n",
        "\n",
        "  return nn.Sequential(\n",
        "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False)\n",
        "  )\n",
        "  upsample(in_coarse)\n",
        "\n",
        "def upsample(ch_coarse, ch_fine):\n",
        "  return nn.Sequential(\n",
        "    nn.ConvTranspose2d(ch_coarse, ch_fine, 4, 2, 1, bias=False),\n",
        "    nn.ReLU()\n",
        "  )\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, useBN=False):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1   = add_conv_stage(1, 32, useBN=useBN)\n",
        "    self.conv2   = add_conv_stage(32, 64, useBN=useBN)\n",
        "    self.conv3   = add_conv_stage(64, 128, useBN=useBN)\n",
        "    self.conv4   = add_conv_stage(128, 256, useBN=useBN)\n",
        "    self.conv5   = add_conv_stage(256, 512, useBN=useBN)\n",
        "\n",
        "    self.conv4m = add_conv_stage(512, 256, useBN=useBN)\n",
        "    self.conv3m = add_conv_stage(256, 128, useBN=useBN)\n",
        "    self.conv2m = add_conv_stage(128,  64, useBN=useBN)\n",
        "    self.conv1m = add_conv_stage( 64,  32, useBN=useBN)\n",
        "\n",
        "    self.conv0  = nn.Sequential(\n",
        "        nn.Conv2d(32, 1, 3, 1, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.max_pool = nn.MaxPool2d(2)\n",
        "\n",
        "    self.upsample54 = upsample(512, 256)\n",
        "    self.upsample43 = upsample(256, 128)\n",
        "    self.upsample32 = upsample(128,  64)\n",
        "    self.upsample21 = upsample(64 ,  32)\n",
        "\n",
        "    ## weight initialization\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    conv1_out = self.conv1(x)\n",
        "    #return self.upsample21(conv1_out)\n",
        "    conv2_out = self.conv2(self.max_pool(conv1_out))\n",
        "    conv3_out = self.conv3(self.max_pool(conv2_out))\n",
        "    conv4_out = self.conv4(self.max_pool(conv3_out))\n",
        "    conv5_out = self.conv5(self.max_pool(conv4_out))\n",
        "\n",
        "    conv5m_out = torch.cat((self.upsample54(conv5_out), conv4_out), 1)\n",
        "    conv4m_out = self.conv4m(conv5m_out)\n",
        "\n",
        "    conv4m_out_ = torch.cat((self.upsample43(conv4m_out), conv3_out), 1)\n",
        "    conv3m_out = self.conv3m(conv4m_out_)\n",
        "\n",
        "    conv3m_out_ = torch.cat((self.upsample32(conv3m_out), conv2_out), 1)\n",
        "    conv2m_out = self.conv2m(conv3m_out_)\n",
        "\n",
        "    conv2m_out_ = torch.cat((self.upsample21(conv2m_out), conv1_out), 1)\n",
        "    conv1m_out = self.conv1m(conv2m_out_)\n",
        "\n",
        "    conv0_out = self.conv0(conv1m_out)\n",
        "\n",
        "    return conv0_out"
      ],
      "metadata": {
        "id": "UjlzVVZyzTfo"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "model = Net().to(device) # try decreasing the depth value if there is a memory error\n"
      ],
      "metadata": {
        "id": "0LDKWHzWz-ta"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model,(1,512,512))"
      ],
      "metadata": {
        "id": "Thm3s3Atw-Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()  # set training mode"
      ],
      "metadata": {
        "id": "6is9udo_0aMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 100\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
        "#lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
        "criterion = nn.MSELoss()\n",
        "# load_checkpoint('/content/drive/MyDrive/torch/DIP/DenoiseAE_0103.pth', model, optimizer)"
      ],
      "metadata": {
        "id": "L_GUUp9W1fTS"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_RGB(img_in):\n",
        "    img_R = model(img_in[:,0,:,:].unsqueeze(1))\n",
        "    img_G = model(img_in[:,1,:,:].unsqueeze(1))\n",
        "    img_B = model(img_in[:,2,:,:].unsqueeze(1))\n",
        "    img_out = torch.cat((img_R,img_G,img_B),dim=1)\n",
        "    return img_out"
      ],
      "metadata": {
        "id": "jiu3fur12n5C"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model):\n",
        "    criterion = nn.MSELoss()\n",
        "    model.eval()  # Important: set evaluation mode\n",
        "    gau_loss, sp_loss, poi_loss = 0,0,0\n",
        "    with torch.no_grad(): # This will free the GPU memory used for back-prop\n",
        "        for origin, gau, sp, poi in val_loader:\n",
        "            origin, gau, sp, poi = origin.cuda(), gau.cuda(), sp.cuda(), poi.cuda()\n",
        "            output = model_RGB(gau)\n",
        "            gau_loss += criterion(output, origin).item()\n",
        "            output = model_RGB(sp)\n",
        "            sp_loss += criterion(output, origin).item()\n",
        "            output = model_RGB(poi)\n",
        "            poi_loss += criterion(output, origin).item()\n",
        "\n",
        "    # l = len(val_set)\n",
        "    # gau_loss /= l\n",
        "    # sp_loss /= l\n",
        "    # poi_loss /= l\n",
        "    print('\\nValidation set: Gaussian loss: {:.4f}, Salt&Pepper loss: {:.4f}, Poisson loss: {:.4f}\\n'.format(gau_loss, sp_loss, poi_loss))"
      ],
      "metadata": {
        "id": "fgS2GAJ_7l-m"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mn = 100\n",
        "for ep in range(epoch):\n",
        "    g,s,p,now = 0,0,0,0\n",
        "    for batch_idx, (origin, gau, sp, poi) in enumerate(train_loader):\n",
        "        origin, gau, sp, poi = origin.cuda(), gau.cuda(), sp.cuda(), poi.cuda()\n",
        "        output1 = model_RGB(gau)\n",
        "        gau_loss = criterion(output1, origin)\n",
        "        output2 = model_RGB(sp)\n",
        "        sp_loss = criterion(output2, origin)\n",
        "        output3 = model_RGB(poi)\n",
        "        poi_loss = criterion(output3, origin)\n",
        "        loss_1, loss_2, loss_3 = criterion(output1, output2),criterion(output1, output3),criterion(output3, output2)\n",
        "        loss = (gau_loss + sp_loss + poi_loss) + (loss_1+loss_2+loss_3)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "        g += gau_loss.item()\n",
        "        s += sp_loss.item()\n",
        "        p += poi_loss.item()\n",
        "\n",
        "    #lr_scheduler.step()\n",
        "    # l = len(train_set)\n",
        "    # g /= l\n",
        "    # s /= l\n",
        "    # p /= l\n",
        "    now = (g+s+p)/3\n",
        "    if now < mn:\n",
        "        mn = now\n",
        "        save_checkpoint('/content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth', model, optimizer)\n",
        "\n",
        "    print('Train Epoch: {} Gaussian loss: {:.4f}, Salt&Pepper loss: {:.4f}, Poisson loss: {:.4f}\\n'.format(ep, g, s, p))\n",
        "    test(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Sd5XOxrE10B8",
        "outputId": "78fa737d-671c-406c-c3ee-c352aa8a6ed7"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model saved to /content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth\n",
            "Train Epoch: 0 Gaussian loss: 0.1032, Salt&Pepper loss: 0.0996, Poisson loss: 0.0782\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0171, Salt&Pepper loss: 0.0147, Poisson loss: 0.0110\n",
            "\n",
            "model saved to /content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth\n",
            "Train Epoch: 1 Gaussian loss: 0.0650, Salt&Pepper loss: 0.0581, Poisson loss: 0.0431\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0170, Salt&Pepper loss: 0.0147, Poisson loss: 0.0109\n",
            "\n",
            "Train Epoch: 2 Gaussian loss: 0.0649, Salt&Pepper loss: 0.0581, Poisson loss: 0.0433\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0169, Salt&Pepper loss: 0.0146, Poisson loss: 0.0110\n",
            "\n",
            "Train Epoch: 3 Gaussian loss: 0.0648, Salt&Pepper loss: 0.0581, Poisson loss: 0.0436\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0169, Salt&Pepper loss: 0.0146, Poisson loss: 0.0111\n",
            "\n",
            "model saved to /content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth\n",
            "Train Epoch: 4 Gaussian loss: 0.0643, Salt&Pepper loss: 0.0577, Poisson loss: 0.0435\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0168, Salt&Pepper loss: 0.0146, Poisson loss: 0.0111\n",
            "\n",
            "Train Epoch: 5 Gaussian loss: 0.0642, Salt&Pepper loss: 0.0577, Poisson loss: 0.0439\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0168, Salt&Pepper loss: 0.0146, Poisson loss: 0.0111\n",
            "\n",
            "Train Epoch: 6 Gaussian loss: 0.0641, Salt&Pepper loss: 0.0575, Poisson loss: 0.0440\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0168, Salt&Pepper loss: 0.0146, Poisson loss: 0.0112\n",
            "\n",
            "Train Epoch: 7 Gaussian loss: 0.0640, Salt&Pepper loss: 0.0575, Poisson loss: 0.0442\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0166, Salt&Pepper loss: 0.0144, Poisson loss: 0.0112\n",
            "\n",
            "Train Epoch: 8 Gaussian loss: 0.0639, Salt&Pepper loss: 0.0573, Poisson loss: 0.0443\n",
            "\n",
            "\n",
            "Validation set: Gaussian loss: 0.0166, Salt&Pepper loss: 0.0143, Poisson loss: 0.0112\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-8d8f409ce978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgau_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msp_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpoi_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate"
      ],
      "metadata": {
        "id": "IbtvwMBZz2k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = DAE().cuda()\n",
        "model.eval()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "load_checkpoint('/content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth', model, optimizer)\n",
        "\n",
        "t2i = transforms.Compose([\n",
        "    #transforms.Resize(256),\n",
        "    #transforms.CenterCrop(512),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "i2t = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    #transforms.Resize((720,720)),\n",
        "])\n",
        "\n",
        "stn = Image.open('blurred/402.png')\n",
        "stn = t2i(stn).cuda()\n",
        "img = Image.open('blurred_noise/402_poisson.png')\n",
        "im_t = t2i(img).cuda()\n",
        "# stn = train_set[305][0].cuda()\n",
        "# im_t = train_set[305][1].cuda()\n",
        "im_o2 = i2t(im_t)\n",
        "print(im_t.shape)\n",
        "stn_t = torch.unsqueeze(stn,0)\n",
        "im_t = torch.unsqueeze(im_t,0)\n",
        "# im_t = im_t.unfold(2, 64, 64).unfold(3, 64, 64)\n",
        "# output = torch.zeros(1,3,768,768)\n",
        "# for i in range(12):\n",
        "#     for j in range(12):\n",
        "#         output[:,:,i*64:(i+1)*64,j*64:(j+1)*64] = model(im_t[:,:,i,j,:,:])\n",
        "#         #output[:,:,i*64:(i+1)*64,j*64:(j+1)*64] = im_t[:,:,i,j,:,:]\n",
        "\n",
        "# for i in range(5):\n",
        "#     for j in range(5):\n",
        "#         im_t[:,:,i*128:(i+1)*128,j*128:(j+1)*128] = model(im_t[:,:,i*128:(i+1)*128,j*128:(j+1)*128])\n",
        "im_t = model_RGB(im_t)\n",
        "\n",
        "loss = criterion(stn_t,im_t)\n",
        "stn_t = torch.squeeze(stn_t,0)\n",
        "stn = i2t(stn_t)\n",
        "im_t = torch.squeeze(im_t,0)\n",
        "im_o = i2t(im_t)"
      ],
      "metadata": {
        "id": "eybgT968_j2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d3333b-5214-4170-d93a-79f2073f33c7"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loaded from /content/drive/MyDrive/torch/DIP/DenoiseAE_UNet_loss.pth\n",
            "torch.Size([3, 720, 720])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im_o.save('denoise.png')\n",
        "im_o2.save('noise.png')\n",
        "stn.save('blurred.png')"
      ],
      "metadata": {
        "id": "bBowIVVuAo10"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScgXNjJP23pO",
        "outputId": "d128212e-7d5f-4a5a-dab4-9190f3d36e8d"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    }
  ]
}